{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/raghavsharma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/raghavsharma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/raghavsharma/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.translate import bleu\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "\n",
    "import openai\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>comment</th>\n",
       "      <th>event</th>\n",
       "      <th>event_player</th>\n",
       "      <th>event_team</th>\n",
       "      <th>comment_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>We hope you have enjoyed our live coverage of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>full time summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>While Madrid's Champions League defence conti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>full time summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>It's all over at Stamford Bridge, and it's an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>full time summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90 + 3</td>\n",
       "      <td>FULL-TIME: CHELSEA 0-2 REAL MADRID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>timer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90 + 3</td>\n",
       "      <td>Valverde sweeps a pass out to Ceballos, who c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>timer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     time                                            comment event  \\\n",
       "0     NaN   We hope you have enjoyed our live coverage of...   NaN   \n",
       "1     NaN   While Madrid's Champions League defence conti...   NaN   \n",
       "2     NaN   It's all over at Stamford Bridge, and it's an...   NaN   \n",
       "3  90 + 3                FULL-TIME: CHELSEA 0-2 REAL MADRID    NaN   \n",
       "4  90 + 3   Valverde sweeps a pass out to Ceballos, who c...   NaN   \n",
       "\n",
       "  event_player event_team       comment_desc  \n",
       "0          NaN        NaN  full time summary  \n",
       "1          NaN        NaN  full time summary  \n",
       "2          NaN        NaN  full time summary  \n",
       "3          NaN        NaN              timer  \n",
       "4          NaN        NaN              timer  "
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.read_csv('../../data/temp_data.csv')\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    \"\"\" A method to remove punctuations from text \"\"\"\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text) #removes numbers from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\" A method to remove all the stopwords \"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    \"\"\" A method to tokenize text data \"\"\"\n",
    "    text = re.split('\\W+', text) #splitting each sentence/ tweet into its individual words\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    \"\"\" A method to perform stemming on text data\"\"\"\n",
    "    porter_stem = nltk.PorterStemmer()\n",
    "    text = [porter_stem.stem(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    word_net_lemma = nltk.WordNetLemmatizer()\n",
    "    text = [word_net_lemma.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a common cleaning function for every part below for code reproducability\n",
    "def clean_text(list_words):\n",
    "    # Making a regex pattern to match in the characters we would like to replace from the words\n",
    "    character_replace = \",()0123456789.?!@#$%&;*:_,/\" \n",
    "    pattern = \"[\" + character_replace + \"]\"\n",
    "    # ------------------------------------------------------------------------------------\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    new_list_words = []\n",
    "    # Looping through every word to remove the characters and appending back to a new list\n",
    "    # replace is being used for the characters that could not be catched through regex\n",
    "    for s in list_words:\n",
    "        new_word = s.lower()\n",
    "        new_word = re.sub(pattern,\"\",new_word)\n",
    "        new_word = new_word.replace('[', '')\n",
    "        new_word = new_word.replace(']', '')\n",
    "        new_word = new_word.replace('-', '')\n",
    "        new_word = new_word.replace('—', '')\n",
    "        new_word = new_word.replace('“', '')\n",
    "        new_word = new_word.replace(\"’\", '')\n",
    "        new_word = new_word.replace(\"”\", '')\n",
    "        new_word = new_word.replace(\"‘\", '')\n",
    "        new_word = new_word.replace('\"', '')\n",
    "        new_word = new_word.replace(\"'\", '')\n",
    "        new_word = new_word.replace(\" \", '')\n",
    "        new_list_words.append(new_word)\n",
    "\n",
    "    # Using filter to remove empty strings\n",
    "    new_list_words = list(filter(None, new_list_words))\n",
    "    return new_list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df, text_col):\n",
    "    \"\"\" A method to do basic data cleaning \"\"\"\n",
    "    \n",
    "    clean_data = df.copy()\n",
    "    \n",
    "    clean_data['clean_text']=clean_data[text_col].apply(lambda x: remove_punct(x))\n",
    "    \n",
    "    clean_data['text_tokenized'] = clean_data['clean_text'].apply(lambda x: tokenization(x.lower()))\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    clean_data['text_without_stop'] = clean_data['text_tokenized'].apply(lambda x: remove_stopwords(x))    \n",
    "    \n",
    "    clean_data['text_stemmed'] = clean_data['text_without_stop'].apply(lambda x: stemming(x))\n",
    "        \n",
    "    clean_data['text_lemmatized'] = clean_data['text_without_stop'].apply(lambda x: lemmatizer(x))\n",
    "\n",
    "    clean_data['text_final'] = clean_data['text_lemmatized'].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(corpus):\n",
    "    \"\"\" A method to do basic data cleaning \"\"\"\n",
    "    clean_data = pd.DataFrame(columns=['ini_text', 'clean_text', 'text_tokenized', 'text_without_stop', \n",
    "                                       'text_stemmed', 'text_lemmatized', 'text_final'])\n",
    "\n",
    "    clean_data['ini_text'] = [corpus]\n",
    "\n",
    "    clean_data['clean_text']=clean_data['ini_text'].apply(lambda x: remove_punct(x))\n",
    "    \n",
    "    clean_data['text_tokenized'] = clean_data['clean_text'].apply(lambda x: tokenization(x.lower()))\n",
    "    \n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    clean_data['text_without_stop'] = clean_data['text_tokenized'].apply(lambda x: remove_stopwords(x))    \n",
    "    \n",
    "    clean_data['text_stemmed'] = clean_data['text_without_stop'].apply(lambda x: stemming(x))\n",
    "        \n",
    "    clean_data['text_lemmatized'] = clean_data['text_without_stop'].apply(lambda x: lemmatizer(x))\n",
    "\n",
    "    clean_data['text_final'] = clean_data['text_lemmatized'].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat the live ticker for 'timer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = ''\n",
    "for comm in temp_df[temp_df['comment_desc'] == 'timer']['comment']:\n",
    "    comment += comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SpaCy to summarize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_spacy_summary(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    keyword = []\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "    for token in doc:\n",
    "        if (token.text in stopwords or token.text in punctuation):\n",
    "            continue\n",
    "        if token.pos_ in pos_tag:\n",
    "            keyword.append(token.text)\n",
    "\n",
    "    freq_word = Counter(keyword)\n",
    "\n",
    "    max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "    for word in freq_word.keys():\n",
    "        freq_word[word] = (freq_word[word]/max_freq)\n",
    "\n",
    "    sent_strenght = {}\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.text in freq_word.keys():\n",
    "                if sent in sent_strenght.keys():\n",
    "                    sent_strenght[sent] += freq_word[word.text]\n",
    "                else:\n",
    "                    sent_strenght[sent] = freq_word[word.text]\n",
    "\n",
    "    summarized_sentences = nlargest(3, sent_strenght, key=sent_strenght.get)\n",
    "    final_sentences = [w.text for w in summarized_sentences]\n",
    "    return \" \".join(final_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text_spacy(text, num_sentences=3):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sentence_scores[i] = sent.similarity(doc)\n",
    "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
    "    summary = [sentences[i].text.strip() for i in top_sentences]\n",
    "    return \" \".join(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using nltk for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text_nltk(text, num_sentences=3):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Tokenize the sentences into words and remove stopwords\n",
    "\n",
    "    # stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    text_punct_removed = remove_punct(text)\n",
    "    words = tokenization(text_punct_removed.lower())\n",
    "    \n",
    "    # words = word_tokenize(text)\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words =  set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming to the filtered words\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "    # Lemmatize words\n",
    "    word_net_lemma = nltk.WordNetLemmatizer()\n",
    "    word_lemma = [word_net_lemma.lemmatize(word) for word in stemmed_words]\n",
    "    \n",
    "    # Calculate word frequency and sentence scores\n",
    "    word_freq = nltk.FreqDist(stemmed_words)\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for word in nltk.word_tokenize(sentence.lower()):\n",
    "            if word in word_freq:\n",
    "                if len(sentence.split()) < 30:\n",
    "                    if i not in sentence_scores:\n",
    "                        sentence_scores[i] = word_freq[word]\n",
    "                    else:\n",
    "                        sentence_scores[i] += word_freq[word]\n",
    "    \n",
    "    # Select the top sentences based on their scores\n",
    "    summary_sentences = nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    summary = [sentences[i] for i in sorted(summary_sentences)]\n",
    "    return \" \".join(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text_gpt(corpus):\n",
    "    openai.organization = 'org-Ik9BKGuegvZTXOQlzGjVSOxz'\n",
    "    openai.api_key = \"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\"\n",
    "    engine_list = openai.Engine.list() # calling the engines available from the openai api \n",
    "    \n",
    "\n",
    "    response = openai.Completion.create(engine=\"text-davinci-003\",prompt=corpus,temperature=0.3,\n",
    "            max_tokens=200,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=[\"\\n\"]\n",
    "        )\n",
    "    return response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In google colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get full time summary using comment description from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" We hope you have enjoyed our live coverage of yet another European win for Real Madrid. It's goodbye for now, and we'll see you next time!   While Madrid's Champions League defence continues apace, Chelsea have lost four successive matches since Frank Lampard returned to the club as interim boss. The Blues will now endure just their second trophyless season in the last seven campaigns, and with 17 points separating them from the Premier League's top four, it could be some time before Stamford Bridge hosts another Champions League fixture!   It's all over at Stamford Bridge, and it's another 2-0 win over Chelsea for Real Madrid – Los Blancos take the quarter-final tie 4-0 on aggregate! Chelsea performed admirably for long periods as Cucurella and Kante missed great chances to cut Madrid's lead, but they could not find a response when Rodrygo hammered Vinicius' cut-back in from close range after 58 minutes. A flowing move involving Valverde led to Rodrygo getting a second with 10 minutes remaining, and Madrid look like they have plenty more to offer as they approach a semi-final tie against Manchester City or Bayern Munich! \""
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_comm = temp_df[temp_df['comment_desc'] == 'full time summary']['comment']\n",
    "full_time_summary = \" \".join(ft_comm)\n",
    "full_time_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize the live ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A rare loose ball from Modric gives Chelsea the chance to put Madrid under pressure, but Kante is crowded out near the edge of the area and Madrid survive. Militao's yellow card means he will miss the first leg of any semi-final tie through suspension, if Madrid make it…  Militao appears to be in some discomfort following that challenge on Gallagher, having kicked the bottom of the Chelsea man's boot as he flew in at a dangerous height. Rodrygo is the target of a clever free-kick into the right-hand channel, but Chalobah and Silva combine to crowd the Brazilian out and win possession for Chelsea.  \""
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach 1\n",
    "spacy_summary1 = get_spacy_summary(comment)\n",
    "spacy_summary1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Vinicius gets the assist for Rodrygo's goal, staying cool under pressure at the far post to pick out his opposite winger, who makes no mistake from close range!  GOAAAAAAAL! James works some space to hit a low cross into the six-yard box, but Alaba slides in to divert it away from the danger zone! A rare loose ball from Modric gives Chelsea the chance to put Madrid under pressure, but Kante is crowded out near the edge of the area and Madrid survive.\""
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach 2\n",
    "spacy_summary2 = summarize_text_spacy(comment)\n",
    "spacy_summary2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' FULL-TIME: CHELSEA 0-2 REAL MADRID  Valverde sweeps a pass out to Ceballos, who cuts inside and shoots straight at Kepa from the edge of the area. A rare loose ball from Modric gives Chelsea the chance to put Madrid under pressure, but Kante is crowded out near the edge of the area and Madrid survive. Cucurella is on hand to intercept a slack ball from Valverde in midfield, and Chelsea can look to build an attack.'"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_summary = summarize_text_nltk(comment)\n",
    "nltk_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" We're ready for the second leg of this Champions League quarter-final! Chelsea, 2-0 down from the first leg, will be hoping for a repeat of their remarkable comeback against Paris Saint-Germain in the last round. Madrid, meanwhile, are looking to extend their record-breaking run of Champions League knockout-stage wins to 13.  TEAM NEWS:  CHELSEA XI: Kepa, James, Silva, Fofana, Cucurella, Kante, Kovacic, Gallagher, Havertz, Chalobah, Fernandez  REAL MADRID XI: Courtois, Carvajal, Militao, Ramos, Alaba, Valverde, Modric, Kroos, Rodrygo, Benzema, Vinicius  We're just moments away from kick-off at Stamford Bridge!\""
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_summary = summarize_text_gpt(comment)\n",
    "gpt_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BLEU for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_bleu(ft_summary, my_summary):\n",
    "#     return bleu([ft_summary.split()], my_summary.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Cosine similarity for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CountVectorizer()\n",
    "def calc_cos_sim_count_vec(ft_summary, my_summary):\n",
    "    corpus = [ft_summary, my_summary]\n",
    "    # Create the Document Term Matrix\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df = pd.DataFrame(doc_term_matrix, \n",
    "                    columns=count_vectorizer.vocabulary_.keys(), \n",
    "                    index=['ft_summary_org','ft_summary_crtd'])\n",
    "    \n",
    "    # Compute Cosine Similarity\n",
    "    return cosine_similarity(df[0:1], df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using TF-IDF\n",
    "# def calc_cos_sim_tfidf(ft_summary, my_summary):\n",
    "#     corpus = [ft_summary, my_summary]\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     trsfm=vectorizer.fit_transform(corpus)\n",
    "\n",
    "#     # OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "#     doc_term_matrix = trsfm.todense()\n",
    "#     trsfm_df = pd.DataFrame(doc_term_matrix,\n",
    "#                             columns=vectorizer.vocabulary_.keys(),\n",
    "#                             index=['ft_summary_org','ft_summary_crtd'])\n",
    "\n",
    "#     return cosine_similarity(trsfm[0:1], trsfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.       , 0.4714833]])"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_cos_sim_count_vec(full_time_summary, spacy_summary1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.41096252]])"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_cos_sim_count_vec(full_time_summary, spacy_summary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.45017869]])"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_cos_sim_count_vec(full_time_summary, nltk_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.47755499]])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_cos_sim_count_vec(full_time_summary, gpt_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('anly521')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "045993bec6224f6bf157e7265b027fdb84d23efb85222067a0580c561b3b8fe4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
