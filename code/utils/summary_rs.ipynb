{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/raghavsharma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/raghavsharma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/raghavsharma/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.translate import bleu\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "\n",
    "import openai\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>comment</th>\n",
       "      <th>event</th>\n",
       "      <th>event_player</th>\n",
       "      <th>event_team</th>\n",
       "      <th>comment_desc</th>\n",
       "      <th>home_team</th>\n",
       "      <th>home_team_abbr</th>\n",
       "      <th>away_team</th>\n",
       "      <th>away_team_abbr</th>\n",
       "      <th>full_time_score</th>\n",
       "      <th>match</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>match_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>thanks for joining our commentary this evenin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>full time summary</td>\n",
       "      <td>barcelona</td>\n",
       "      <td>BAR</td>\n",
       "      <td>bayern munchen</td>\n",
       "      <td>FCB</td>\n",
       "      <td>0 - 3</td>\n",
       "      <td>Barcelona vs Bayern Muenchen</td>\n",
       "      <td>09/14/21</td>\n",
       "      <td>https://www.goal.com/en/match/barcelona-vs-bay...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>barcelona are next in action at home in lalig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>full time summary</td>\n",
       "      <td>barcelona</td>\n",
       "      <td>BAR</td>\n",
       "      <td>bayern munchen</td>\n",
       "      <td>FCB</td>\n",
       "      <td>0 - 3</td>\n",
       "      <td>Barcelona vs Bayern Muenchen</td>\n",
       "      <td>09/14/21</td>\n",
       "      <td>https://www.goal.com/en/match/barcelona-vs-bay...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>bayern munich have eased past barcelona in th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>full time summary</td>\n",
       "      <td>barcelona</td>\n",
       "      <td>BAR</td>\n",
       "      <td>bayern munchen</td>\n",
       "      <td>FCB</td>\n",
       "      <td>0 - 3</td>\n",
       "      <td>Barcelona vs Bayern Muenchen</td>\n",
       "      <td>09/14/21</td>\n",
       "      <td>https://www.goal.com/en/match/barcelona-vs-bay...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90 + 2</td>\n",
       "      <td>full-time: barcelona 0-3 bayern munich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>timer</td>\n",
       "      <td>barcelona</td>\n",
       "      <td>BAR</td>\n",
       "      <td>bayern munchen</td>\n",
       "      <td>FCB</td>\n",
       "      <td>0 - 3</td>\n",
       "      <td>Barcelona vs Bayern Muenchen</td>\n",
       "      <td>09/14/21</td>\n",
       "      <td>https://www.goal.com/en/match/barcelona-vs-bay...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>there will be two minutes of added time.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>timer</td>\n",
       "      <td>barcelona</td>\n",
       "      <td>BAR</td>\n",
       "      <td>bayern munchen</td>\n",
       "      <td>FCB</td>\n",
       "      <td>0 - 3</td>\n",
       "      <td>Barcelona vs Bayern Muenchen</td>\n",
       "      <td>09/14/21</td>\n",
       "      <td>https://www.goal.com/en/match/barcelona-vs-bay...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     time                                            comment event  \\\n",
       "0     NaN   thanks for joining our commentary this evenin...   NaN   \n",
       "1     NaN   barcelona are next in action at home in lalig...   NaN   \n",
       "2     NaN   bayern munich have eased past barcelona in th...   NaN   \n",
       "3  90 + 2            full-time: barcelona 0-3 bayern munich    NaN   \n",
       "4      90          there will be two minutes of added time.    NaN   \n",
       "\n",
       "  event_player event_team       comment_desc  home_team home_team_abbr  \\\n",
       "0          NaN        NaN  full time summary  barcelona            BAR   \n",
       "1          NaN        NaN  full time summary  barcelona            BAR   \n",
       "2          NaN        NaN  full time summary  barcelona            BAR   \n",
       "3          NaN        NaN              timer  barcelona            BAR   \n",
       "4          NaN        NaN              timer  barcelona            BAR   \n",
       "\n",
       "        away_team away_team_abbr full_time_score  \\\n",
       "0  bayern munchen            FCB           0 - 3   \n",
       "1  bayern munchen            FCB           0 - 3   \n",
       "2  bayern munchen            FCB           0 - 3   \n",
       "3  bayern munchen            FCB           0 - 3   \n",
       "4  bayern munchen            FCB           0 - 3   \n",
       "\n",
       "                          match      date  \\\n",
       "0  Barcelona vs Bayern Muenchen  09/14/21   \n",
       "1  Barcelona vs Bayern Muenchen  09/14/21   \n",
       "2  Barcelona vs Bayern Muenchen  09/14/21   \n",
       "3  Barcelona vs Bayern Muenchen  09/14/21   \n",
       "4  Barcelona vs Bayern Muenchen  09/14/21   \n",
       "\n",
       "                                                link  match_id  \n",
       "0  https://www.goal.com/en/match/barcelona-vs-bay...         0  \n",
       "1  https://www.goal.com/en/match/barcelona-vs-bay...         0  \n",
       "2  https://www.goal.com/en/match/barcelona-vs-bay...         0  \n",
       "3  https://www.goal.com/en/match/barcelona-vs-bay...         0  \n",
       "4  https://www.goal.com/en/match/barcelona-vs-bay...         0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.read_csv('../../data/commentory_matchid.csv')\n",
    "temp_df = temp_df[temp_df['match_id'] != 95]\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    \"\"\" A method to remove punctuations from text \"\"\"\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text) #removes numbers from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\" A method to remove all the stopwords \"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    \"\"\" A method to tokenize text data \"\"\"\n",
    "    text = re.split('\\W+', text) #splitting each sentence/ tweet into its individual words\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    \"\"\" A method to perform stemming on text data\"\"\"\n",
    "    porter_stem = nltk.PorterStemmer()\n",
    "    text = [porter_stem.stem(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    word_net_lemma = nltk.WordNetLemmatizer()\n",
    "    text = [word_net_lemma.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a common cleaning function for every part below for code reproducability\n",
    "def clean_text(list_words):\n",
    "    # Making a regex pattern to match in the characters we would like to replace from the words\n",
    "    character_replace = \",()0123456789.?!@#$%&;*:_,/\" \n",
    "    pattern = \"[\" + character_replace + \"]\"\n",
    "    # ------------------------------------------------------------------------------------\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    new_list_words = []\n",
    "    # Looping through every word to remove the characters and appending back to a new list\n",
    "    # replace is being used for the characters that could not be catched through regex\n",
    "    for s in list_words:\n",
    "        new_word = s.lower()\n",
    "        new_word = re.sub(pattern,\"\",new_word)\n",
    "        new_word = new_word.replace('[', '')\n",
    "        new_word = new_word.replace(']', '')\n",
    "        new_word = new_word.replace('-', '')\n",
    "        new_word = new_word.replace('—', '')\n",
    "        new_word = new_word.replace('“', '')\n",
    "        new_word = new_word.replace(\"’\", '')\n",
    "        new_word = new_word.replace(\"”\", '')\n",
    "        new_word = new_word.replace(\"‘\", '')\n",
    "        new_word = new_word.replace('\"', '')\n",
    "        new_word = new_word.replace(\"'\", '')\n",
    "        new_word = new_word.replace(\" \", '')\n",
    "        new_list_words.append(new_word)\n",
    "\n",
    "    # Using filter to remove empty strings\n",
    "    new_list_words = list(filter(None, new_list_words))\n",
    "    return new_list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df, text_col):\n",
    "    \"\"\" A method to do basic data cleaning \"\"\"\n",
    "    \n",
    "    clean_data = df.copy()\n",
    "    \n",
    "    clean_data['clean_text']=clean_data[text_col].apply(lambda x: remove_punct(x))\n",
    "    \n",
    "    clean_data['text_tokenized'] = clean_data['clean_text'].apply(lambda x: tokenization(x.lower()))\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    clean_data['text_without_stop'] = clean_data['text_tokenized'].apply(lambda x: remove_stopwords(x))    \n",
    "    \n",
    "    clean_data['text_stemmed'] = clean_data['text_without_stop'].apply(lambda x: stemming(x))\n",
    "        \n",
    "    clean_data['text_lemmatized'] = clean_data['text_without_stop'].apply(lambda x: lemmatizer(x))\n",
    "\n",
    "    clean_data['text_final'] = clean_data['text_lemmatized'].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(corpus):\n",
    "    \"\"\" A method to do basic data cleaning \"\"\"\n",
    "    clean_data = pd.DataFrame(columns=['ini_text', 'clean_text', 'text_tokenized', 'text_without_stop', \n",
    "                                       'text_stemmed', 'text_lemmatized', 'text_final'])\n",
    "\n",
    "    clean_data['ini_text'] = [corpus]\n",
    "\n",
    "    clean_data['clean_text']=clean_data['ini_text'].apply(lambda x: remove_punct(x))\n",
    "    \n",
    "    clean_data['text_tokenized'] = clean_data['clean_text'].apply(lambda x: tokenization(x.lower()))\n",
    "    \n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    clean_data['text_without_stop'] = clean_data['text_tokenized'].apply(lambda x: remove_stopwords(x))    \n",
    "    \n",
    "    clean_data['text_stemmed'] = clean_data['text_without_stop'].apply(lambda x: stemming(x))\n",
    "        \n",
    "    clean_data['text_lemmatized'] = clean_data['text_without_stop'].apply(lambda x: lemmatizer(x))\n",
    "\n",
    "    clean_data['text_final'] = clean_data['text_lemmatized'].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract commentary for every match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comm(df):\n",
    "    all_comm = []\n",
    "    match_ids = df['match_id'].unique()\n",
    "    for id in match_ids:\n",
    "        # Filter the dataframe w.r.t match_id\n",
    "        match_df = df[df['match_id'] == id]\n",
    "        match_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "        comment = ''\n",
    "        for comm in match_df[match_df['comment_desc'] == 'timer']['comment']:\n",
    "            comment += comm\n",
    "        all_comm.append(comment)    \n",
    "    return all_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comm = extract_comm(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat the live ticker for 'timer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = ''\n",
    "for comm in temp_df[temp_df['comment_desc'] == 'timer']['comment']:\n",
    "    comment += comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SpaCy to summarize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_spacy_summary(text):\n",
    "    doc = nlp(text)\n",
    "    keyword = []\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "    for token in doc:\n",
    "        if (token.text in stopwords or token.text in punctuation):\n",
    "            continue\n",
    "        if token.pos_ in pos_tag:\n",
    "            keyword.append(token.text)\n",
    "    \n",
    "    if len(keyword) != 0:\n",
    "        freq_word = Counter(keyword)\n",
    "        max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "        for word in freq_word.keys():\n",
    "            freq_word[word] = (freq_word[word]/max_freq)\n",
    "\n",
    "        sent_strenght = {}\n",
    "        for sent in doc.sents:\n",
    "            for word in sent:\n",
    "                if word.text in freq_word.keys():\n",
    "                    if sent in sent_strenght.keys():\n",
    "                        sent_strenght[sent] += freq_word[word.text]\n",
    "                    else:\n",
    "                        sent_strenght[sent] = freq_word[word.text]\n",
    "\n",
    "        summarized_sentences = nlargest(3, sent_strenght, key=sent_strenght.get)\n",
    "        final_sentences = [w.text for w in summarized_sentences]\n",
    "        return \" \".join(final_sentences)\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text_spacy(text, num_sentences=3):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sentence_scores[i] = sent.similarity(doc)\n",
    "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
    "    summary = [sentences[i].text.strip() for i in top_sentences]\n",
    "    return \" \".join(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using nltk for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text_nltk(text, num_sentences=3):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Tokenize the sentences into words and remove stopwords\n",
    "\n",
    "    # stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    text_punct_removed = remove_punct(text)\n",
    "    words = tokenization(text_punct_removed.lower())\n",
    "    \n",
    "    # words = word_tokenize(text)\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words =  set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming to the filtered words\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "    # Lemmatize words\n",
    "    word_net_lemma = nltk.WordNetLemmatizer()\n",
    "    word_lemma = [word_net_lemma.lemmatize(word) for word in stemmed_words]\n",
    "    \n",
    "    # Calculate word frequency and sentence scores\n",
    "    word_freq = nltk.FreqDist(stemmed_words)\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for word in nltk.word_tokenize(sentence.lower()):\n",
    "            if word in word_freq:\n",
    "                if len(sentence.split()) < 30:\n",
    "                    if i not in sentence_scores:\n",
    "                        sentence_scores[i] = word_freq[word]\n",
    "                    else:\n",
    "                        sentence_scores[i] += word_freq[word]\n",
    "    \n",
    "    # Select the top sentences based on their scores\n",
    "    summary_sentences = nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    summary = [sentences[i] for i in sorted(summary_sentences)]\n",
    "    return \" \".join(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text_gpt(corpus, org_key, api_key):\n",
    "    openai.organization = org_key\n",
    "    openai.api_key = api_key\n",
    "    models = openai.Model.list()\n",
    "    for model in models['data']:\n",
    "        print(model['id'])\n",
    "    engine_list = openai.Engine.list() # calling the engines available from the openai api \n",
    "\n",
    "    response = openai.Completion.create(engine=\"davinci\",prompt=corpus,temperature=0.3,\n",
    "            max_tokens=200,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=[\"\\n\"]\n",
    "        )\n",
    "    return response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9401"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_comm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babbage\n",
      "davinci\n",
      "text-davinci-edit-001\n",
      "babbage-code-search-code\n",
      "text-similarity-babbage-001\n",
      "code-davinci-edit-001\n",
      "text-davinci-001\n",
      "ada\n",
      "babbage-code-search-text\n",
      "babbage-similarity\n",
      "code-search-babbage-text-001\n",
      "text-curie-001\n",
      "code-search-babbage-code-001\n",
      "text-ada-001\n",
      "text-embedding-ada-002\n",
      "text-similarity-ada-001\n",
      "curie-instruct-beta\n",
      "ada-code-search-code\n",
      "ada-similarity\n",
      "code-search-ada-text-001\n",
      "text-search-ada-query-001\n",
      "davinci-search-document\n",
      "ada-code-search-text\n",
      "text-search-ada-doc-001\n",
      "davinci-instruct-beta\n",
      "text-similarity-curie-001\n",
      "code-search-ada-code-001\n",
      "ada-search-query\n",
      "text-search-davinci-query-001\n",
      "curie-search-query\n",
      "davinci-search-query\n",
      "babbage-search-document\n",
      "ada-search-document\n",
      "text-search-curie-query-001\n",
      "whisper-1\n",
      "text-search-babbage-doc-001\n",
      "curie-search-document\n",
      "text-search-curie-doc-001\n",
      "babbage-search-query\n",
      "text-babbage-001\n",
      "text-search-davinci-doc-001\n",
      "gpt-3.5-turbo-0301\n",
      "text-search-babbage-query-001\n",
      "curie-similarity\n",
      "gpt-3.5-turbo\n",
      "curie\n",
      "text-davinci-003\n",
      "text-similarity-davinci-001\n",
      "text-davinci-002\n",
      "davinci-similarity\n",
      "cushman:2020-05-03\n",
      "ada:2020-05-03\n",
      "babbage:2020-05-03\n",
      "curie:2020-05-03\n",
      "davinci:2020-05-03\n",
      "if-davinci-v2\n",
      "if-curie-v2\n",
      "if-davinci:3.0.0\n",
      "davinci-if:3.0.0\n",
      "davinci-instruct-beta:2.0.0\n",
      "text-ada:001\n",
      "text-davinci:001\n",
      "text-curie:001\n",
      "text-babbage:001\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "The model: `davinci-codex` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m api_key \u001b[38;5;241m=\u001b[39m api[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m org_key \u001b[38;5;241m=\u001b[39m api[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m gpt_comm \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_text_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_comm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morg_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m gpt_comm\n",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m, in \u001b[0;36msummarize_text_gpt\u001b[0;34m(corpus, org_key, api_key)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m engine_list \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mEngine\u001b[38;5;241m.\u001b[39mlist() \u001b[38;5;66;03m# calling the engines available from the openai api \u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdavinci-codex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/anly521/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/anly521/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniconda3/envs/anly521/lib/python3.10/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/envs/anly521/lib/python3.10/site-packages/openai/api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    614\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    615\u001b[0m         )\n\u001b[1;32m    616\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    617\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 620\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    623\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    624\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    627\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/anly521/lib/python3.10/site-packages/openai/api_requestor.py:683\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    681\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    682\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 683\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    684\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: The model: `davinci-codex` does not exist"
     ]
    }
   ],
   "source": [
    "# reading keys from file\n",
    "\n",
    "api = pd.read_csv('../../../OpenAI_st.txt')\n",
    "\n",
    "api_key = api[\"Key\"][0]\n",
    "org_key = api[\"Key\"][1]\n",
    "gpt_comm = summarize_text_gpt(all_comm[0], org_key, api_key)\n",
    "gpt_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In google colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get full time summary using comment description from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" We hope you have enjoyed our live coverage of yet another European win for Real Madrid. It's goodbye for now, and we'll see you next time!   While Madrid's Champions League defence continues apace, Chelsea have lost four successive matches since Frank Lampard returned to the club as interim boss. The Blues will now endure just their second trophyless season in the last seven campaigns, and with 17 points separating them from the Premier League's top four, it could be some time before Stamford Bridge hosts another Champions League fixture!   It's all over at Stamford Bridge, and it's another 2-0 win over Chelsea for Real Madrid – Los Blancos take the quarter-final tie 4-0 on aggregate! Chelsea performed admirably for long periods as Cucurella and Kante missed great chances to cut Madrid's lead, but they could not find a response when Rodrygo hammered Vinicius' cut-back in from close range after 58 minutes. A flowing move involving Valverde led to Rodrygo getting a second with 10 minutes remaining, and Madrid look like they have plenty more to offer as they approach a semi-final tie against Manchester City or Bayern Munich! \""
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_comm = temp_df[temp_df['comment_desc'] == 'full time summary']['comment']\n",
    "full_time_summary = \" \".join(ft_comm)\n",
    "full_time_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize the live ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A rare loose ball from Modric gives Chelsea the chance to put Madrid under pressure, but Kante is crowded out near the edge of the area and Madrid survive. Militao's yellow card means he will miss the first leg of any semi-final tie through suspension, if Madrid make it…  Militao appears to be in some discomfort following that challenge on Gallagher, having kicked the bottom of the Chelsea man's boot as he flew in at a dangerous height. Rodrygo is the target of a clever free-kick into the right-hand channel, but Chalobah and Silva combine to crowd the Brazilian out and win possession for Chelsea.  \""
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach 1\n",
    "spacy_summary1 = get_spacy_summary(comment)\n",
    "spacy_summary1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Vinicius gets the assist for Rodrygo's goal, staying cool under pressure at the far post to pick out his opposite winger, who makes no mistake from close range!  GOAAAAAAAL! James works some space to hit a low cross into the six-yard box, but Alaba slides in to divert it away from the danger zone! A rare loose ball from Modric gives Chelsea the chance to put Madrid under pressure, but Kante is crowded out near the edge of the area and Madrid survive.\""
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach 2\n",
    "spacy_summary2 = summarize_text_spacy(comment)\n",
    "spacy_summary2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' FULL-TIME: CHELSEA 0-2 REAL MADRID  Valverde sweeps a pass out to Ceballos, who cuts inside and shoots straight at Kepa from the edge of the area. A rare loose ball from Modric gives Chelsea the chance to put Madrid under pressure, but Kante is crowded out near the edge of the area and Madrid survive. Cucurella is on hand to intercept a slack ball from Valverde in midfield, and Chelsea can look to build an attack.'"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_summary = summarize_text_nltk(comment)\n",
    "nltk_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" We're just moments away from kick-off at Stamford Bridge, with Chelsea needing to overturn a two-goal deficit to keep their Champions League campaign alive.  Madrid have won each of their last four Champions League knockout ties against English sides, while they have also kept a clean sheet in each of their last three meetings with Chelsea.  Chelsea have made two changes from their last outing, with Kovacic and Gallagher coming in for Mount and Azpilicueta. Madrid, meanwhile, have made just one alteration, with Carvajal replacing Nacho at right-back.  TEAM NEWS:  CHELSEA XI: Kepa; Cucurella, Silva, Fofana, James; Kovacic, Chalobah; Gallagher, Havertz, Fernandez; Kante  REAL MADRID XI: Courtois; Carvajal, Militao, Alaba, Valverde; Modric, Casemiro, Kroos; Rod\""
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_summary = summarize_text_gpt(comment)\n",
    "gpt_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BLEU for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_bleu(ft_summary, my_summary):\n",
    "#     return bleu([ft_summary.split()], my_summary.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Cosine similarity for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CountVectorizer()\n",
    "def calc_cos_sim_count_vec(ft_summary, my_summary):\n",
    "    corpus = [ft_summary, my_summary]\n",
    "    # Create the Document Term Matrix\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df = pd.DataFrame(doc_term_matrix, \n",
    "                    columns=count_vectorizer.vocabulary_.keys(), \n",
    "                    index=['ft_summary_org','ft_summary_crtd'])\n",
    "    \n",
    "    # Compute Cosine Similarity\n",
    "    return cosine_similarity(df[0:1], df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using TF-IDF\n",
    "# def calc_cos_sim_tfidf(ft_summary, my_summary):\n",
    "#     corpus = [ft_summary, my_summary]\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     trsfm=vectorizer.fit_transform(corpus)\n",
    "\n",
    "#     # OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "#     doc_term_matrix = trsfm.todense()\n",
    "#     trsfm_df = pd.DataFrame(doc_term_matrix,\n",
    "#                             columns=vectorizer.vocabulary_.keys(),\n",
    "#                             index=['ft_summary_org','ft_summary_crtd'])\n",
    "\n",
    "#     return cosine_similarity(trsfm[0:1], trsfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.       , 0.4714833]])"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_cos_sim_count_vec(full_time_summary, spacy_summary1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.41096252]])"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_cos_sim_count_vec(full_time_summary, spacy_summary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.45017869]])"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_cos_sim_count_vec(full_time_summary, nltk_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.43443679]])"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_cos_sim_count_vec(full_time_summary, gpt_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('anly521')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "045993bec6224f6bf157e7265b027fdb84d23efb85222067a0580c561b3b8fe4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
